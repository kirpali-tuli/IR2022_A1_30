{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-A1-Q1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "V8_2BHfwZROt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6519e12-143e-4eb5-f637-e3d4dfc8760b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import regex as re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import collections\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xLY4EYNwmKND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba925a87-6673-4c98-fc6c-16e610a865f2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "W5Q_GThBzTvR"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(a) Carry out the suitable preprocessing steps on the given dataset.*"
      ],
      "metadata": {
        "id": "x4BuWKMtc8Iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_index = {}  #term-doc_name pairs\n",
        "\n",
        "def preprocess_doc_and_get_terms(doc, doc_name):\n",
        "    doc = doc.lower()\n",
        "    doc = re.sub(r'[^\\w\\s]', ' ', doc)\n",
        "    words = nltk.word_tokenize(doc)\n",
        "    words = [\"\".join(ch for ch in word if ch.isalpha()) for word in words]\n",
        "    words = {lem.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))}\n",
        "    words = list(words)\n",
        "    for j in range(len(words)):\n",
        "        if words[j] in inverted_index.keys():\n",
        "            inverted_index[words[j]].add(doc_name)\n",
        "        else:\n",
        "            inverted_index[words[j]] = {doc_name}\n",
        "    return words"
      ],
      "metadata": {
        "id": "0n9SucEHkn0F"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = os.fsencode(\"/content/drive/MyDrive/IR Assignments/Humor,Hist,Media,Food\")\n",
        "    \n",
        "doc_text = []\n",
        "doc_names = []\n",
        "doc_preprocessed_words = []\n",
        "\n",
        "for doc_name in os.listdir(directory):\n",
        "    doc_path = os.path.join(directory,doc_name)\n",
        "    doc = open(doc_path, encoding=\"utf-8\", errors=\"surrogateescape\")\n",
        "    raw_doc = doc.read()\n",
        "    doc_text.append(raw_doc)\n",
        "    doc_names.append(doc_name)\n",
        "    doc_preprocessed_words.append(preprocess_doc_and_get_terms(raw_doc,doc_name))\n",
        "\n",
        "corpus = pd.DataFrame(list(zip(doc_names, doc_text, doc_preprocessed_words)), columns = ['doc_name', 'doc_text', 'doc_words'])\n",
        "corpus.head()"
      ],
      "metadata": {
        "id": "IFh1ODbJayED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2f663460-25f5-4e5c-9cc5-9fcf07cb660e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1b06cc3d-301c-4234-af72-6f1e202f49d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_name</th>\n",
              "      <th>doc_text</th>\n",
              "      <th>doc_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'talkbizr.txt'</td>\n",
              "      <td>Newsgroups: talk.bizarre\\nFrom: rigler@dao.nrc...</td>\n",
              "      <td>[, fbi, followup, chaos, paper, gamepiece, ssc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'takenote.jok'</td>\n",
              "      <td>From davidt@psuhcx Tue Aug  2 22:30:02 1988\\nF...</td>\n",
              "      <td>[, form, said, psuhcx, ever, tue, lucidity, de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'sysadmin.txt'</td>\n",
              "      <td>From: szielins@us.oracle.com (szielins.US1)\\nN...</td>\n",
              "      <td>[, cobol, bit, jagermeister, biff, maniacal, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'taping.hum'</td>\n",
              "      <td>\\t\\t\\t  [The TAPING I by: UNDERWAREZ.]\\n\\n    ...</td>\n",
              "      <td>[, wait, yeah, hmmm, pizza, jeepers, mind, ope...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'temphell.jok'</td>\n",
              "      <td>The temperature of Heaven can be rather accura...</td>\n",
              "      <td>[, vol, stefan, hotter, unbelieving, mean, sev...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b06cc3d-301c-4234-af72-6f1e202f49d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b06cc3d-301c-4234-af72-6f1e202f49d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b06cc3d-301c-4234-af72-6f1e202f49d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          doc_name  ...                                          doc_words\n",
              "0  b'talkbizr.txt'  ...  [, fbi, followup, chaos, paper, gamepiece, ssc...\n",
              "1  b'takenote.jok'  ...  [, form, said, psuhcx, ever, tue, lucidity, de...\n",
              "2  b'sysadmin.txt'  ...  [, cobol, bit, jagermeister, biff, maniacal, b...\n",
              "3    b'taping.hum'  ...  [, wait, yeah, hmmm, pizza, jeepers, mind, ope...\n",
              "4  b'temphell.jok'  ...  [, vol, stefan, hotter, unbelieving, mean, sev...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.to_csv('corpus.csv', encoding=\"utf-8\", errors=\"surrogateescape\")"
      ],
      "metadata": {
        "id": "FMHbfduoRAVf"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxEIdjutQRJw",
        "outputId": "650b8ce4-1688-4f57-abb8-123488afb40a"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1133 entries, 0 to 1132\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   doc_name   1133 non-null   object\n",
            " 1   doc_text   1133 non-null   object\n",
            " 2   doc_words  1133 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 26.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(inverted_index.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vm2G70HvI_5",
        "outputId": "1c4f1dca-2cb7-4065-b677-1e01d665373d"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for k, v in inverted_index.items():\n",
        "#     print(k, v)"
      ],
      "metadata": {
        "id": "U3jCtBvmvM6u"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(b) Implement the unigram inverted index data structure.*"
      ],
      "metadata": {
        "id": "k0yPXzRqdPkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_inverted_index = collections.OrderedDict(sorted(inverted_index.items()))"
      ],
      "metadata": {
        "id": "lB3D9sYrwTuR"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_inverted_index = dict(sorted_inverted_index)  # POSTINGS LIST\n",
        "# for k, v in sorted_inverted_index.items():\n",
        "#     print(k, v)"
      ],
      "metadata": {
        "id": "H_xWAL4X-aqO"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sorted_inverted_index.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYga_C6U-dfg",
        "outputId": "39e3298a-394c-4471-aae4-dafe24f0c9c3"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58479"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in sorted_inverted_index.keys():\n",
        "    sorted_inverted_index[key] = sorted(sorted_inverted_index[key]) #sorted() returns a list"
      ],
      "metadata": {
        "id": "2uBdr592-zkB"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(sorted_inverted_index['world'])\n",
        "# print(len(sorted_inverted_index['world']))"
      ],
      "metadata": {
        "id": "2DcuBn1VORJB"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(c) Provide support for the following queries-*\n",
        "```\n",
        "(i) x OR y\n",
        "(ii) x AND y\n",
        "(iii) x AND NOT y\n",
        "(iv) x OR NOT y\n",
        "```"
      ],
      "metadata": {
        "id": "DhsMq0bLddzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_query(query):\n",
        "    query = query.lower()\n",
        "    query = re.sub(r'[^\\w\\s]', ' ', query)\n",
        "    words = nltk.word_tokenize(query)\n",
        "    words = [\"\".join(ch for ch in word if ch.isalpha()) for word in words]\n",
        "    words = {lem.lemmatize(word) for word in words if word not in set(stopwords.words(\"english\"))}\n",
        "    words = list(words)\n",
        "    return words"
      ],
      "metadata": {
        "id": "TVYEv3H32QKQ"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OR(docs1, docs2):\n",
        "    docs = []\n",
        "    comparisons = 0\n",
        "\n",
        "    while(len(docs1) > 0 and len(docs2) > 0):\n",
        "        if(docs1[0] < docs2[0]):\n",
        "            comparisons = comparisons+1\n",
        "            docs.append(docs1[0])\n",
        "            docs1 = docs1[1:]\n",
        "        elif(docs1[0] > docs2[0]):\n",
        "            comparisons = comparisons+1\n",
        "            docs.append(docs2[0])\n",
        "            docs2 = docs2[1:]\n",
        "        else:\n",
        "            comparisons = comparisons+1\n",
        "            docs.append(docs1[0])\n",
        "            docs1 = docs1[1:]\n",
        "            docs2 = docs2[1:]\n",
        "    while(len(docs1) > 0):\n",
        "        docs.append(docs1[0])\n",
        "        docs1 = docs1[1:]\n",
        "    while(len(docs2) > 0):\n",
        "        docs.append(docs2[0])\n",
        "        docs2 = docs2[1:]\n",
        "\n",
        "    return docs, comparisons\n",
        "\n",
        "def AND(docs1, docs2):\n",
        "    docs = []\n",
        "    comparisons = 0\n",
        "\n",
        "    while(len(docs1) > 0 and len(docs2) > 0):\n",
        "        if(docs1[0] > docs2[0]):\n",
        "            comparisons = comparisons+1\n",
        "            docs2 = docs2[1:]\n",
        "        elif(docs1[0] < docs2[0]):\n",
        "            comparisons = comparisons+1\n",
        "            docs1 = docs1[1:]\n",
        "        else:\n",
        "            comparisons = comparisons+1\n",
        "            docs.append(docs1[0])\n",
        "            docs1 = docs1[1:]\n",
        "            docs2 = docs2[1:]\n",
        "\n",
        "    return docs, comparisons\n",
        "\n",
        "def AND_NOT(docs1, docs2):\n",
        "    common_docs, comparisons = AND(docs1.copy(), docs2.copy())\n",
        "    docs = []\n",
        "\n",
        "    while(len(docs1) > 0):\n",
        "        if(len(common_docs) == 0):\n",
        "            docs.append(docs1[0])\n",
        "            docs1 = docs1[1:]\n",
        "        elif(docs1[0] == common_docs[0]):\n",
        "            docs1 = docs1[1:]\n",
        "            common_docs = common_docs[1:]\n",
        "            comparisons = comparisons+1\n",
        "        elif(docs1[0] < common_docs[0]):\n",
        "            docs.append(docs1[0])\n",
        "            docs1 = docs1[1:]\n",
        "            comparisons = comparisons+1\n",
        "\n",
        "    return docs, comparisons\n",
        "\n",
        "def OR_NOT(docs1,docs2):\n",
        "    docs_not_having_2 = []\n",
        "    comparisons_all = 0\n",
        "\n",
        "    for a in corpus['doc_name']:\n",
        "        if a not in docs2:\n",
        "            docs_not_having_2.append(a)\n",
        "            comparisons_all = comparisons_all+1\n",
        "    docs_not_having_2 = sorted(docs_not_having_2)\n",
        "\n",
        "    docs, comparisons_or = OR(docs1.copy(),docs_not_having_2.copy())\n",
        "\n",
        "    return docs, comparisons_all + comparisons_or"
      ],
      "metadata": {
        "id": "fCx3nTmdgSph"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query, operators):\n",
        "    total_comparisons = 0\n",
        "\n",
        "    if(len(query) != len(operators)+1):\n",
        "        print(\"Invalid Query!!!\")\n",
        "        return\n",
        "    else:\n",
        "        for i, op in enumerate(operators):\n",
        "            if i == 0:\n",
        "                if op == \"AND\":\n",
        "                    docs, comparisons = AND(sorted_inverted_index[query[i]].copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                elif op == \"OR\":\n",
        "                    docs, comparisons = OR(sorted_inverted_index[query[i]].copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                elif op == \"AND NOT\":\n",
        "                    docs, comparisons = AND_NOT(sorted_inverted_index[query[i]].copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                elif op == \"OR NOT\":\n",
        "                    docs, comparisons = OR_NOT(sorted_inverted_index[query[i]].copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                else:\n",
        "                    print(\"Invalid Query!!!\")\n",
        "                    return\n",
        "            else:\n",
        "                if op == \"AND\":\n",
        "                    docs, comparisons = AND(docs.copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                elif op == \"OR\":\n",
        "                    docs, comparisons = OR(docs.copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                elif op == \"AND NOT\":\n",
        "                    docs, comparisons = AND_NOT(docs.copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                elif op == \"OR NOT\":\n",
        "                    docs, comparisons = OR_NOT(docs.copy(),sorted_inverted_index[query[i+1]].copy())\n",
        "                else:\n",
        "                    print(\"Invalid Query!!!\")\n",
        "                    return\n",
        "            total_comparisons += comparisons\n",
        "            \n",
        "    return docs, total_comparisons"
      ],
      "metadata": {
        "id": "5RMQW3oC7dHB"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*(d) Evaluation against some queries in the format given below*\n",
        "```\n",
        "Input Format:\n",
        "\n",
        "The first line contains the number of queries, N.\n",
        "The next 2N lines would represent the queries.\n",
        "Each query would consist of two lines:\n",
        "(a) line 1: Input sentence\n",
        "(b) line 2: Input operation sequence\n",
        "\n",
        "Output Format:\n",
        "\n",
        "• The number of documents retrieved.\n",
        "• The minimum number of total comparisons done (if any)( only in merging algorithm).\n",
        "• The list of document names retrieved.\n",
        "```"
      ],
      "metadata": {
        "id": "Gy-NnIUuLmRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = int(input(\"Enter the number of queries: \"))\n",
        "\n",
        "for _ in range(N):\n",
        "    query = input(\"Enter input sequence: \")\n",
        "    query = preprocess_query(str(query))\n",
        "    operators = input(\"Enter operation sequence: \")\n",
        "    operators = operators.upper().split(\", \")\n",
        "    # print(query)\n",
        "    # print(operators)\n",
        "\n",
        "    docs, comparisons = process_query(query, operators)\n",
        "    \n",
        "    print(\"Number of documents matched: \",len(docs))\n",
        "    print(\"No. of comparisons required: \",comparisons)\n",
        "    print(\"list of documents matched: \",docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgeiuz0GxKnP",
        "outputId": "337c3c93-d769-4399-f459-d6eaf33a0e89"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter input sequence: lion stood peacefully for a moment\n",
            "Enter operation sequence: or, or, or\n",
            "Number of documents matched:  211\n",
            "No. of comparisons required:  471\n",
            "list of documents matched:  [b'a_tv_t-p.com', b'aeonint.txt', b'allusion', b'ambrose.bie', b'anim_lif.txt', b'anime.lif', b'annoy.fascist', b'art-fart.hum', b'b-2.jok', b'badday.hum', b'barney.txt', b'bbh_intv.txt', b'beauty.tm', b'beesherb.txt', b'bitnet.txt', b'bmdn01.txt', b'boneles2.txt', b'booze1.fun', b'butwrong.hum', b'bw-phwan.hat', b'bw.txt', b'cabbage.txt', b'caesardr.sal', b'calculus.txt', b'candy.txt', b'cartoon.law', b'cartoon.laws', b'cartoon_.txt', b'chickenheadbbs.txt', b'childhoo.jok', b'christop.int', b'clancy.txt', b'classicm.hum', b'cmu.share', b'cogdis.txt', b'collected_quotes.txt', b'commutin.jok', b'computer.txt', b'conan.txt', b'consp.txt', b'cookie.1', b'coyote.txt', b'cuchy.hum', b'cybrtrsh.txt', b'dead3.txt', b'dead4.txt', b'dead5.txt', b'deep.txt', b'dingding.hum', b'doggun.sto', b'drinks.gui', b'dromes.txt', b'drunk.txt', b'dthought.txt', b'econridl.fun', b'engineer.hum', b'english.txt', b'epi_.txt', b'epi_tton.txt', b'episimp2.txt', b'epitaph', b'eskimo.nel', b'exam.50', b'facedeth.txt', b'fascist.txt', b'female.jok', b'filmgoof.txt', b'flux_fix.txt', b'fuckyou2.txt', b'gas.txt', b'gd_gal.txt', b'gd_ql.txt', b'ghostfun.hum', b'golnar.txt', b'gown.txt', b'grail.txt', b'hackingcracking.txt', b'hackmorality.txt', b'hecomes.jok', b'hermsys.txt', b'homebrew.txt', b'humor9.txt', b'idr2.txt', b'incarhel.hum', b'indgrdn.txt', b'initials.rid', b'insult.lst', b'insults1.txt', b'iremember', b'is_story.txt', b'ivan.hum', b'japantv.txt', b'jayjay.txt', b'jc-elvis.inf', b'kaboom.hum', b'kanalx.txt', b'lawyer.jok', b'lbinter.hum', b'let.go', b'letgosh.txt', b'lif&love.hum', b'lifeimag.hum', b'lifeonledge.txt', b'lion.jok', b'lion.txt', b'lions.cat', b'llong.hum', b'lozerzon.hum', b'luggage.hum', b'luvstory.txt', b'm0dzmen.hum', b'maecenas.hum', b'mailfrag.hum', b'manners.txt', b'marriage.hum', b'mcd.txt', b'meinkamp.hum', b'mel.txt', b'mindvox', b'minn.txt', b'misc.1', b'mlverb.hum', b'montpyth.hum', b'moore.txt', b'moose.txt', b'msorrow', b'mundane.v2', b'murphy_l.txt', b'murphys.txt', b'myheart.hum', b'namaste.txt', b'nameisreo.txt', b'news.hum', b'nigel.10', b'nigel.2', b'nigel.3', b'nigel.5', b'nigel10.txt', b'nihgel_8.9', b'nukewar.txt', b'oldeng.hum', b'oliver.txt', b'oliver02.txt', b'onetoone.hum', b'onetotwo.hum', b'oxymoron.jok', b'passage.hum', b'passenge.sim', b'peatchp.hum', b'pepper.txt', b'pepsideg.txt', b'petshop', b'phorse.hum', b'pizzawho.hum', b'policpig.hum', b'popmusi.hum', b'prac1.jok', b'prac2.jok', b'prac3.jok', b'prac4.jok', b'pracjoke.txt', b'practica.txt', b'pro-fact.hum', b'progrs.gph', b'psych_pr.quo', b'psycho.txt', b'pukeprom.jok', b'puzzles.jok', b'quack26.txt', b'quest.hum', b'quotes.txt', b'quux_p.oem', b'radiolaf.hum', b'reasons.txt', b'reeves.txt', b'rns_ency.txt', b'scratchy.txt', b'sfmovie.txt', b'shuttleb.hum', b'smurfkil.hum', b'snapple.rum', b'socecon.hum', b'socks.drx', b'solders.hum', b'soleleer.hum', b'stone.hum', b'strine.txt', b'stuf10.txt', b'stuf11.txt', b'suicide2.txt', b'sw_err.txt', b\"terrmcd'.hum\", b'tfepisod.hum', b'three.txt', b'throwawa.hum', b'timetr.hum', b'tnd.1', b'top10.txt', b'top10st2.txt', b'tpquotes.txt', b'ukunderg.txt', b'valujet.txt', b'various.txt', b'vonthomp', b'wacky.ani', b'wagon.hum', b'wedding.hum', b'whoops.hum', b'wimptest.txt', b'worldend.hum', b'xibovac.txt']\n",
            "Enter input sequence: telephone, paved, roads\n",
            "Enter operation sequence: and not, or not\n",
            "Number of documents matched:  1127\n",
            "No. of comparisons required:  2506\n",
            "list of documents matched:  [b'1st_aid.txt', b'a-team', b'a_fish_c.apo', b'a_tv_t-p.com', b'abbott.txt', b'aboutada.txt', b'acetab1.txt', b'aclamt.txt', b'acne1.txt', b'acronym.lis', b'acronym.txt', b'acronyms.txt', b'adameve.hum', b'adcopy.hum', b'addrmeri.txt', b'admin.txt', b'adrian_e.faq', b'ads.txt', b'adt_miam.txt', b'advrtize.txt', b'aeonint.txt', b'age.txt', b'aggie.txt', b'aids.txt', b'airlines', b'alabama.txt', b'alcatax.txt', b'alcohol.hum', b'alflog.txt', b'all_grai', b'allfam.epi', b'allusion', b'amazing.epi', b'ambrose.bie', b'amchap2.txt', b'analogy.hum', b'aniherb.txt', b'anim_lif.txt', b'anime.cli', b'anime.lif', b'annoy.fascist', b'anorexia.txt', b'answers', b'anthropo.stu', b'antibiot.txt', b'antimead.bev', b'aphrodis.txt', b'appbred.brd', b'appetiz.rcp', b'applepie.des', b'apsaucke.des', b'apsnet.txt', b'arab.dic', b'arcadian.txt', b'argotdic.txt', b'arnold.txt', b'art-fart.hum', b'arthriti.txt', b'ateam.epi', b'atherosc.txt', b'atombomb.hum', b'att.txt', b'aussie.lng', b'avengers.lis', b'awespinh.sal', b'ayurved.txt', b'b-2.jok', b'b12.txt', b'back1.txt', b'bad', b'bad-d', b'bad.jok', b'badday.hum', b'bagelope.txt', b'bakebred.txt', b'baklava.des', b'banana01.brd', b'banana02.brd', b'banana03.brd', b'banana04.brd', b'banana05.brd', b'bank.rob', b'barney.cn1', b'basehead.txt', b'batrbred.txt', b'bb', b'bbc_vide.cat', b'bbh_intv.txt', b'bbq.txt', b'beapimp.hum', b'beauty.tm', b'beave.hum', b'beer-g', b'beer-gui', b'beer.gam', b'beer.hum', b'beer.txt', b'beerdiag.txt', b'beergame.hum', b'beergame.txt', b'beerjesus.hum', b'beershrm.fis', b'beershrp.fis', b'beerwarn.txt', b'beesherb.txt', b'beginn.ers', b'berryeto.bev', b'bhang.fun', b'bhb.ill', b'bible.txt', b'bigpic1.hum', b'billcat.hum', b'bimg.prn', b'bingbong.hum', b'bitchcar.hum', b'bitnet.txt', b'blackadd', b'blackapp.hum', b'blackhol.hum', b'blake7.lis', b'blaster.hum', b'bless.bc', b'blkbean.txt', b'blkbnsrc.vgn', b'blood.txt', b'blooprs1.asc', b'bmdn01.txt', b'bnb_quot.txt', b'bnbeg2.4.txt', b'bnbguide.txt', b'boarchil.txt', b'boatmemo.jok', b'boe.hum', b'bond-2.txt', b'boneles2.txt', b'booknuti.txt', b'booze.fun', b'booze1.fun', b'booze2.fun', b'bored.txt', b'boston.geog', b'bozo_tv.leg', b'brainect.hum', b'brdpudd.des', b'bread.rcp', b'bread.rec', b'bread.txt', b'breadpud.des', b'bredcake.des', b'brewing', b'browneco.hum', b'brownie.rec', b'brush1.txt', b'btaco.txt', b'btcisfre.hum', b'btscke01.des', b'btscke02.des', b'btscke03.des', b'btscke04.des', b'btscke05.des', b'buffwing.pol', b'bugbreak.hum', b'bugs.txt', b'buldrwho.txt', b'bunacald.fis', b'burrito.mea', b'butcher.txt', b'butstcod.fis', b'butwrong.hum', b'buzzword.hum', b'bw-phwan.hat', b'bw-summe.hat', b'bw.txt', b'byfb.txt', b'c0dez.txt', b'cabbage.txt', b'caesardr.sal', b'cake.rec', b'calamus.hrb', b'calculus.txt', b'calif.hum', b'calvin.txt', b'cancer.rat', b'candy.txt', b'candybar.fun', b'capital.txt', b'caramels.des', b'carowner.txt', b'cars.txt', b'cartoon.law', b'cartoon.laws', b'cartoon_.txt', b'cartoon_laws.txt', b'cartwb.son', b'cast.lis', b'catballs.hum', b'catin.hat', b'catranch.hum', b'catstory.txt', b'cbmatic.hum', b'cereal.txt', b'cform2.txt', b'cgs_lst.txt', b'chainltr.txt', b'change.hum', b'charity.hum', b'cheapfar.hum', b'cheapin.la', b'chickenheadbbs.txt', b'chickens.jok', b'chickens.txt', b'childhoo.jok', b'childrenbooks.txt', b'chili.txt', b'chinese.txt', b'chinesec.hum', b'choco-ch.ips', b'christop.int', b'chung.iv', b'chunnel.txt', b'church.sto', b'clancy.txt', b'classicm.hum', b'climbing.let', b'cmu.share', b'co-car.jok', b'cockney.alp', b'coffee.faq', b'coffee.txt', b'coffeebeerwomen.txt', b'cogdis.txt', b'coke.fun', b'coke.txt', b'coke1', b'coke_fan.naz', b'cokeform.txt', b'coladrik.fun', b'coladrik.txt', b'cold.fus', b'coldfake.hum', b'collected_quotes.txt', b'college.hum', b'college.sla', b'college.txt', b'comic_st.gui', b'commutin.jok', b'commword.hum', b'computer.txt', b'comrevi1.hum', b'conan.txt', b'confucius_say.txt', b'consp.txt', b'contract.moo', b'cookberk', b'cookbkly.how', b'cookie.1', b'cooking.fun', b'cooking.jok', b'coollngo2.txt', b'cooplaws', b'cops.txt', b'corporat.txt', b'court.quips', b'cowexplo.hum', b'coyote.txt', b'crazy.txt', b'critic.txt', b'crzycred.lst', b'cuchy.hum', b'cucumber.jok', b'cucumber.txt', b'cuisine.txt', b'cultmov.faq', b'curiousgeorgie.txt', b'curry.hrb', b'curry.txt', b'curse.txt', b'cybrtrsh.txt', b'd-ned.hum', b'dalive', b'damiana.hrb', b'dandwine.bev', b'dark.suc', b'dead-r', b'dead2.txt', b'dead3.txt', b'dead4.txt', b'dead5.txt', b'deadlysins.txt', b'deathhem.txt', b'deep.txt', b'defectiv.hum', b'desk.txt', b'deterior.hum', b'devils.jok', b'diesmurf.txt', b'diet.txt', b'dieter.txt', b'dingding.hum', b'dining.out', b'dirtword.txt', b'disaster.hum', b'disclmr.txt', b'disclym.txt', b'doc-says.txt', b'docdict.txt', b'docspeak.txt', b'doggun.sto', b'donut.txt', b'dover.poem', b'draxamus.txt', b'drinker.txt', b'drinking.tro', b'drinkrul.jok', b'drinks.gui', b'drinks.txt', b'drive.txt', b'dromes.txt', b'druggame.hum', b'drugshum.hum', b'drunk.txt', b'dthought.txt', b'dubltalk.jok', b'dym', b'eandb.drx', b'earp', b'eatme.txt', b'econridl.fun', b'egg-bred.txt', b'egglentl.vgn', b'eggroll1.mea', b'electric.txt', b'element.jok', b'elephant.fun', b'elevator.fun', b'empeval.txt', b'engineer.hum', b'english', b'english.txt', b'engmuffn.txt', b'engrhyme.txt', b'enlightenment.txt', b'enquire.hum', b'epi_.txt', b'epi_bnb.txt', b'epi_merm.txt', b'epi_rns.txt', b'epi_tton.txt', b'epikarat.txt', b'epiquest.txt', b'episimp2.txt', b'epitaph', b'eskimo.nel', b'exam.50', b'excuse.txt', b'excuse30.txt', b'excuses.txt', b'exidy.txt', b'exylic.txt', b'f_tang.txt', b'facedeth.txt', b'failure.txt', b'fajitas.rcp', b'farsi.phrase', b'farsi.txt', b'fartinfo.txt', b'fartting.txt', b'fascist.txt', b'fbipizza.txt', b'fearcola.hum', b'fed.txt', b'fegg!int.txt', b'feggaqui.txt', b'feggmagi.txt', b'feista01.dip', b'female.jok', b'fiber.txt', b'figure_1.txt', b'filmgoof.txt', b'films_gl.txt', b'final-ex.txt', b'finalexm.hum', b'firecamp.txt', b'fireplacein.txt', b'firstaid.inf', b'firstaid.txt', b'fish.rec', b'flattax.hum', b'flowchrt', b'flowchrt.txt', b'flux_fix.txt', b'focaccia.brd', b'food', b'foodtips', b'footfun.hum', b'forsooth.hum', b'free-cof.fee', b'freshman.hum', b'freudonseuss.txt', b'frogeye1.sal', b'from.hum', b'fuck!.txt', b'fuckyou2.txt', b'fudge.txt', b'fusion.gal', b'fusion.sup', b'fwksfun.hum', b'gack!.txt', b'gaiahuma', b'gameshow.txt', b'ganamembers.txt', b'garlpast.vgn', b'gas.txt', b'gd_alf.txt', b'gd_drwho.txt', b'gd_flybd.txt', b'gd_frasr.txt', b'gd_gal.txt', b'gd_guide.txt', b'gd_hhead.txt', b'gd_liqtv.txt', b'gd_maxhd.txt', b'gd_ol.txt', b'gd_ql.txt', b'gd_sgrnd.txt', b'gd_tznew.txt', b'german.aut', b'get.drunk.cheap', b'ghostfun.hum', b'ghostsch.hum', b'gingbeer.txt', b'girlspeak.txt', b'godmonth.txt', b'goforth.hum', b'gohome.hum', b'goldwatr.txt', b'golnar.txt', b'good.txt', b'gotukola.hrb', b'gown.txt', b'grail.txt', b'grammar.jok', b'greenchi.txt', b'grommet.hum', b'grospoem.txt', b'growth.txt', b'gumbo.txt', b'hack', b'hack7.txt', b'hackingcracking.txt', b'hackmorality.txt', b'hacktest.txt', b'hamburge.nam', b'hammock.hum', b'hangover.txt', b'happyhack.txt', b'harmful.hum', b'hate.hum', b'hbo_spec.rev', b'headlnrs', b'hecomes.jok', b'hedgehog.txt', b'height.txt', b'hell.jok', b'hell.txt', b'herb!.hum', b'hermsys.txt', b'heroic.txt', b'hi.tec', b'hierarch.txt', b'highland.epi', b'hilbilly.wri', b'history2.oop', b'hitchcoc.app', b'hitchcok.txt', b'hitler.59', b'hitler.txt', b'hitlerap.txt', b'homebrew.txt', b'homermmm.txt', b'hoonsrc.txt', b'hop.faq', b'horflick.txt', b'horoscop.jok', b'horoscop.txt', b'horoscope.txt', b'hotel.txt', b'hotnnot.hum', b'hotpeper.txt', b'how.bugs.breakd', b'how2bgod.txt', b'how2dotv.txt', b'how_to_i.pro', b'howlong.hum', b'htswfren.txt', b'hum2', b'humatra.txt', b'humatran.jok', b'humpty.dumpty', b'iced.tea', b'icm.hum', b'idaho.txt', b'idr2.txt', b'imbecile.txt', b'imprrisk.hum', b'impurmat.hum', b'incarhel.hum', b'indgrdn.txt', b'initials.rid', b'inlaws1.txt', b'inquirer.txt', b'ins1', b'insanity.hum', b'insect1.txt', b'insult', b'insults1.txt', b'insuranc.sty', b'insure.hum', b'interv.hum', b'investi.hum', b'iqtest', b'iremember', b'is_story.txt', b'italoink.txt', b'ivan.hum', b'jac&tuu.hum', b'jalapast.dip', b'jambalay.pol', b'japantv.txt', b'japice.bev', b'japrap.hum', b'jargon.phd', b'jason.fun', b'jawgumbo.fis', b'jawsalad.fis', b'jayjay.txt', b'jc-elvis.inf', b'jeffie.heh', b'jerky.rcp', b'jimhood.txt', b'johann', b'jokeju07.txt', b'jokes', b'jokes.txt', b'jokes1.txt', b'jon.txt', b'jrrt.riddle', b'jungjuic.bev', b'just2', b'justify', b'kaboom.hum', b'kanalx.txt', b'kashrut.txt', b'kid2', b'kid_diet.txt', b'killer.hum', b'killself.hum', b'kilroy', b'kilsmur.hum', b'kloo.txt', b'koans.txt', b'la_times.hun', b'labels.txt', b'lampoon.jok', b'languag.jok', b'lansing.txt', b'law.sch', b'lawhunt.txt', b'laws.txt', b'lawskool.txt', b'lawsuniv.hum', b'lawyer.jok', b'lawyers.txt', b'lazarus.txt', b'lbinter.hum', b'leech.txt', b'legal.hum', b'let.go', b'letgosh.txt', b'letter.txt', b'letter_f.sch', b'letterbx.txt', b'libraway.txt', b'liceprof.sty', b'lif&love.hum', b'lifeimag.hum', b'lifeinfo.hum', b'lifeonledge.txt', b'limerick.jok', b'lines.jok', b'lion.jok', b'lion.txt', b'lions.cat', b'lipkovits.txt', b'livnware.hum', b'llamas.txt', b'lll.hum', b'llong.hum', b'lobquad.hum', b'looser.hum', b'losers84.hum', b'losers86.hum', b'lost.txt', b'lotsa.jok', b'lozers', b'lozerzon.hum', b'lozeuser.hum', b'lp-assoc.txt', b'lucky.cha', b'ludeinfo.hum', b'ludeinfo.txt', b'luggage.hum', b'luvstory.txt', b'luzerzo2.hum', b'm0dzmen.hum', b'macsfarm.old', b'madhattr.jok', b'madscrib.hum', b'maecenas.hum', b'mailfrag.hum', b'makebeer.hum', b'making_y.wel', b'malechem.txt', b'manager.txt', b'manilla.hum', b'manners.txt', b'manspace.hum', b'margos.txt', b'marines.hum', b'marriage.hum', b'mash.hum', b'math.1', b'math.2', b'math.far', b'maxheadr', b'mcd.txt', b'mead.rcp', b'meat2.txt', b'meinkamp.hum', b'mel.txt', b'melodram.hum', b'memo.hum', b'memory.hum', b'men&wome.txt', b'mensroom.jok', b'merry.txt', b'miamadvi.hum', b'miami.hum', b'miamimth.txt', b'middle.age', b'minn.txt', b'miranda.hum', b'misc.1', b'misery.hum', b'missdish', b'missheav.hum', b'mitch.txt', b'mlverb.hum', b'modemwld.txt', b'modest.hum', b'modstup', b'mog-history', b'montoys.txt', b'montpyth.hum', b'moonshin', b'moore.txt', b'moose.txt', b'moslem.txt', b'mothers.txt', b'motrbike.jok', b'mov_rail.txt', b'mowers.txt', b'mr.rogers', b'mrscienc.hum', b'mrsfield', b'msfields.txt', b'msorrow', b'mtm.hum', b'mtv.asc', b'mundane.v2', b'murph.jok', b'murphy.txt', b'murphy_l.txt', b'murphys.txt', b'mutate.hum', b'mydaywss.hum', b'myheart.hum', b'naivewiz.hum', b'namaste.txt', b'nameisreo.txt', b'namm', b'nasaglenn.txt', b'necropls.txt', b'netmask.txt', b'netnews.10', b'newcoke.txt', b'newconst.hum', b'newmex.hum', b'news.hum', b'nigel.1', b'nigel.10', b'nigel.2', b'nigel.3', b'nigel.4', b'nigel.5', b'nigel.6', b'nigel.7', b'nigel10.txt', b'nihgel_8.9', b'nintendo.jok', b'normal.boy', b'normalboy.txt', b'normquot.txt', b'nosuch_nasfic', b'novel.hum', b'nuke.hum', b'nukeplay.hum', b'nukewar.jok', b'nukewar.txt', b'nukwaste', b'number', b'number.killer', b'number_k.ill', b'nurds.hum', b'nysucks.hum', b'nzdrinks.txt', b'o-ttalk.hum', b'oakwood.txt', b'oam-001.txt', b'oam.nfo', b'oasis', b'oatbran.rec', b'oculis.rcp', b'odd_to.obs', b'odearakk.hum', b'office.txt', b'ohandre.hum', b'oilgluts.hum', b'old.txt', b'oldeng.hum', b'oldtime.sng', b'oldtime.txt', b'oliver.txt', b'oliver02.txt', b'onan.txt', b'one.par', b'onetoone.hum', b'onetotwo.hum', b'ookpik.hum', b'opinion.hum', b'oracle.jok', b'oranchic.pol', b'orgfrost.bev', b'ourfathr.txt', b'outawork.erl', b'outlimit.txt', b'oxymoron.jok', b'oxymoron.txt', b'ozarks.hum', b'p-law.hum', b'packard.txt', b'paddingurpapers.txt', b'parabl.hum', b'parades.hum', b'parsnip.txt', b'passage.hum', b'passenge.sim', b'pasta001.sal', b'pat.txt', b'pbcookie.des', b'peanuts.txt', b'peatchp.hum', b'pecker.txt', b'penisprt.txt', b'penndtch', b'pepper.txt', b'pepsideg.txt', b'petshop', b'phony.hum', b'phorse.hum', b'phunatdi.ana', b'phxbbs-m.txt', b'pickup.lin', b'pickup.txt', b'pipespec.txt', b'pizzawho.hum', b'planeget.hum', b'planetzero.txt', b'poets.hum', b'pol-corr.txt', b'polemom.txt', b'poli.tics', b'poli_t.ics', b'policpig.hum', b'poll2res.hum', b'polly.txt', b'polly_.new', b'poopie.txt', b'popconc.hum', b'popmach', b'popmusi.hum', b'post.nuc', b'pot.txt', b'potty.txt', b'pournell.spo', b'ppbeer.txt', b'prac1.jok', b'prac2.jok', b'prac3.jok', b'prac4.jok', b'pracjoke.txt', b'practica.txt', b'prawblim.hum', b'prayer.hum', b'primes.jok', b'princess.brd', b'pro-fact.hum', b'problem.txt', b'progrs.gph', b'proof.met', b'prooftec.txt', b'proposal.jok', b'proudlyserve.txt', b'prover.wisom', b'prover_w.iso', b'psalm.reagan', b'psalm23.txt', b'psalm_nixon', b'psalm_re.aga', b'psilaine.hum', b'psych_pr.quo', b'psycho.txt', b'pukeprom.jok', b'pun.txt', b'pure.mat', b'puzzle.spo', b'puzzles.jok', b'python_s.ong', b'q.pun', b'qttofu.vgn', b'quack26.txt', b'quantity.001', b'quantum.jok', b'quantum.phy', b'quest.hum', b'quick.jok', b'quotes.bug', b'quotes.jok', b'quotes.txt', b'quux_p.oem', b'rabbit.txt', b'racist.net', b'radexposed.txt', b'radiolaf.hum', b'rapmastr.hum', b'ratings.hum', b'ratspit.hum', b'raven.hum', b'readme.bat', b'reagan.hum', b'realest.txt', b'reasons.txt', b'rec.por', b'recepies.fun', b'recip1.txt', b'recipe.001', b'recipe.002', b'recipe.003', b'recipe.004', b'recipe.005', b'recipe.006', b'recipe.007', b'recipe.008', b'recipe.009', b'recipe.010', b'recipe.011', b'recipe.012', b'reconcil.hum', b'record_.gap', b'red-neck.jks', b'reddwarf.sng', b'reddye.hum', b'rednecks.txt', b'reeves.txt', b'relative.ada', b'religion.txt', b'renored.txt', b'renorthr.txt', b'rent-a_cat', b'rentals.hum', b'repair.hum', b'report.hum', b'research.hum', b'residncy.jok', b'resolutn.txt', b'resrch_p.hra', b'resrch_phrase', b'revolt.dj', b'richbred.txt', b'rinaldo.jok', b'rinaldos.law', b'rinaldos.txt', b'ripoffpc.hum', b'rns_bcl.txt', b'rns_bwl.txt', b'rns_ency.txt', b'roach.asc', b'roadpizz.txt', b'robot.tes', b'rocking.hum', b'rockmus.hum', b'sanshop.txt', b'saveface.hum', b'sawyer.txt', b'scam.txt', b'scratchy.txt', b'seafood.txt', b'seeds42.txt', b'sf-zine.pub', b'sfmovie.txt', b'shameonu.hum', b'shooters.txt', b'shorties.jok', b'shrink.news', b'shuimai.txt', b'shuttleb.hum', b'signatur.jok', b'sigs.txt', b'silverclaws.txt', b'simp.txt', b'sinksub.txt', b'skincat', b'skippy.hum', b'skippy.txt', b'slogans.txt', b'smackjok.hum', b'smartass.txt', b'smiley.txt', b'smokers.txt', b'smurf-03.txt', b'smurf_co.txt', b'smurfkil.hum', b'smurfs.cc', b'snapple.rum', b'snipe.txt', b'soccer.txt', b'socecon.hum', b'social.hum', b'socks.drx', b'solders.hum', b'soleleer.hum', b'solviets.hum', b'some_hu.mor', b'soporifi.abs', b'sorority.gir', b'spacever.hum', b'spelin_r.ifo', b'speling.msk', b'spider.hum', b'spoonlis.txt', b'spydust.hum', b'squids.gph', b'st_silic.txt', b'staff.txt', b'stagline.txt', b'standard.hum', b'startrek.txt', b'stereo.txt', b'steroid.txt', b'stone.hum', b'strattma.txt', b'stressman.txt', b'strine.txt', b'strsdiet.txt', b'studentb.txt', b'stuf10.txt', b'stuf11.txt', b'subb_lis.txt', b'subrdead.hum', b'suicide2.txt', b'sungenu.hum', b'supermar.rul', b'sw_err.txt', b'swearfrn.hum', b'symbol.hum', b'sysadmin.txt', b'sysman.txt', b't-10.hum', b't-shirt.hum', b't_zone.jok', b'takenote.jok', b'talebeat.hum', b'talkbizr.txt', b'taping.hum', b'tarot.txt', b'teens.txt', b'teevee.hum', b'telecom.q', b'televisi.hum', b'televisi.txt', b'temphell.jok', b'terbear.txt', b'termpoem.txt', b'terms.hum', b\"terrmcd'.hum\", b'terrnieg.hum', b'test.hum', b'test.jok', b'test2.jok', b'testchri.txt', b'texbeef.txt', b'texican.dic', b'texican.lex', b'textgrap.hum', b'tfepisod.hum', b'tfpoems.hum', b'the_ant.txt', b'the_math.hel', b'thecube.hum', b'thermite.ana', b'thesis.beh', b'thievco.txt', b'three.txt', b'throwawa.hum', b'tickmoon.hum', b'timetr.hum', b'tnd.1', b'top10.elf', b'top10.txt', b'top10st1.txt', b'top10st2.txt', b'topten.hum', b'toxcwast.hum', b'tpquote2.txt', b'tpquotes.txt', b'transp.txt', b'trekfume.txt', b'trekwes.hum', b'tribble.hum', b'trukdeth.txt', b'truthlsd.hum', b'truths.hum', b'tshirts.jok', b'tuflife.txt', b'tuna.lab', b'turbo.hum', b'turing.shr', b'turkey.fun', b'twilight.txt', b'twinkie.txt', b'twinkies.jok', b'twinpeak.txt', b'ukunderg.txt', b'un.happy', b'units.mea', b'univ.odd', b'unochili.txt', b'urban.txt', b'vaguemag.90s', b'valujet.txt', b'variety1.asc', b'variety2.asc', b'variety3.asc', b'various.txt', b'vegan.rcp', b'vegkill.txt', b'venganza.txt', b'venison.txt', b'voltron.hum', b'vonthomp', b'wacky.ani', b'wagit.txt', b'wagon.hum', b'waitress.txt', b'washroom.txt', b'watchlip.hum', b'wedding.hum', b'weight.txt', b'weights.hum', b'welfare', b'welfare.txt', b'wetdream.hum', b'whatbbs', b'whatthe.hum', b'whitbred.txt', b'who.txt', b'whoon1st.hum', b'whoops.hum', b'why-me.hum', b'widows', b'wimptest.txt', b'wisconsi.txt', b'wisdom', b'wkrp.epi', b'women.jok', b'wonton.txt', b'wood', b'woodbine.txt', b'woodbugs.txt', b'woods.txt', b'woodsmok.txt', b'woolly_m.amm', b'word.hum', b'worldend.hum', b'wrdnws1.txt', b'wrdnws2.txt', b'wrdnws3.txt', b'wrdnws4.txt', b'wrdnws5.txt', b'wrdnws6.txt', b'wrdnws7.txt', b'wrdnws8.txt', b'wrdnws9.txt', b'x-drinks.txt', b'xibovac.txt', b'xtermin8.hum', b'y.txt', b'yjohncse.hum', b'yogisays.txt', b'yogurt.asc', b'yuban.txt', b'yuppies.hum', b'zen.txt', b'zgtoilet.txt', b'zodiac.hum', b'zucantom.sal', b'zuccmush.sal']\n"
          ]
        }
      ]
    }
  ]
}