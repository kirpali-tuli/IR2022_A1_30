{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_A1_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na10F0OR13Eo",
        "outputId": "c8fbe571-38b9-443f-ef63-d7b8626426b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "import regex as re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from natsort import natsorted"
      ],
      "metadata": {
        "id": "nfOKAXs015WA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZU-7X_c2CmW",
        "outputId": "82873d3e-8b72-404e-9490-f0c81107e966"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posInd = {} #datastructure to store positional index \n",
        "mapping={} #maps fileno -> filename \n",
        "docNo = 0 #represents docId"
      ],
      "metadata": {
        "id": "fJ4WElTc2FfT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing**"
      ],
      "metadata": {
        "id": "PS_kQqL_ltFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to preprocess the file and convert into token list\n",
        "def preprocess_doc(raw_file):\n",
        "    \n",
        "    raw_file = raw_file.lower() #lower-casing\n",
        "    token_list = TweetTokenizer().tokenize(raw_file) #tokenize\n",
        "    stoppers = set(stopwords.words('english')) \n",
        "    token_list = {word for word in token_list if word not in stoppers} #removing stop-words\n",
        "    token_list = [re.sub(r'[^\\w\\s]','',x) for x in token_list] #removing punctuations\n",
        "    token_list = [word for word in token_list if word] #removing blank words\n",
        "    \n",
        "\n",
        "    return token_list"
      ],
      "metadata": {
        "id": "XHKDeTCm2SJk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Indexing**"
      ],
      "metadata": {
        "id": "5NIZrH_llxoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filesPath = '/content/drive/MyDrive/IR Assignment/Humor,Hist,Media,Food/Humor,Hist,Media,Food'\n",
        "files = os.listdir(filesPath)\n",
        "#for each file\n",
        "for f in files:\n",
        "  docPath = os.path.join(filesPath,f)\n",
        "  myDoc = open(docPath,encoding='utf-8',errors='surrogateescape').read()\n",
        "  myTokens = preprocess_doc(myDoc) #pre-process file\n",
        "\n",
        "  #positional indexing :: Refer https://www.geeksforgeeks.org/python-positional-index/\n",
        "  for pos,name in enumerate(myTokens):\n",
        "    if name in posInd: #if token already present in ds\n",
        "      posInd[name][0] = posInd[name][0]+1  #incr freq\n",
        "\n",
        "      if docNo in posInd[name][1]: #if fileno already present then add into pos list\n",
        "        posInd[name][1][docNo].append(pos)\n",
        "      else:\n",
        "        posInd[name][1][docNo] = [pos] #create new list\n",
        "    else: #create new list and dict\n",
        "      posInd[name] = []\n",
        "      posInd[name].append(1)\n",
        "      posInd[name].append({})\n",
        "      posInd[name][1][docNo] = [pos]\n",
        "  \n",
        "  mapping[docNo]= f; #maps file no\n",
        "  docNo=docNo+1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q_tmLAlX4Kc7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Processing**"
      ],
      "metadata": {
        "id": "qYuZeIONdwKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to merge tokens and find common docs for phrase tokens\n",
        "def find_docs(w1,w2,dist):\n",
        "  output=[]\n",
        "  l1 = len(w1)\n",
        "  l2 = len(w2)\n",
        "  i1=j1=0\n",
        "  while i1<l1 and j1<l2: #till length of doc no found\n",
        "    if w1[i1][0] == w2[j1][0]: #matched doc-id\n",
        "      \n",
        "      ll1 = (w1[i1][1])\n",
        "      ll2 = (w2[j1][1])\n",
        "      i2=j2=0\n",
        "      postlist=[]\n",
        "      while i2<len(ll1) and j2<len(ll2): #if pos of first < sec then add to pos list\n",
        "          if (ll2[j2]>ll1[i2] and (ll2[j2]-ll1[i2])<=dist):\n",
        "            postlist.append(ll2[j2])\n",
        "            i2=i2+1\n",
        "            j2=j2+1\n",
        "          elif ll2[j2]<ll1[i2]: #incr second pos list ptr\n",
        "            j2=j2+1\n",
        "          else:\n",
        "            i2=i2+1\n",
        "      \n",
        "        \n",
        "      output.append([w1[i1][0],postlist])\n",
        "      i1=i1+1\n",
        "      j1=j1+1\n",
        "    elif w1[i1][0] < w2[j1][0]:\n",
        "      i1=i1+1\n",
        "    else:\n",
        "      j1=j1+1\n",
        "  return output "
      ],
      "metadata": {
        "id": "_7datkw1d8uV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=input(\"Enter Query::\")\n",
        "tokens = preprocess_doc(query)\n",
        "query = query.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQzloyh9hsf6",
        "outputId": "e8697fbb-2fd8-4102-9bda-d327521e41e0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Query::tripped over the garden\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d1=[]\n",
        "dict1 = posInd[tokens[0]][1]\n",
        "for k,v in dict1.items(): #convert dict into req ds\n",
        "    temp = [k,v]\n",
        "    d1.append(temp)\n",
        "for i in range(1,len(tokens)): #for each token in query\n",
        "  \n",
        "  d2=[]\n",
        "  dict2 = posInd[tokens[i]][1]\n",
        "  for k,v in dict2.items(): #convert dict into req ds\n",
        "    temp = [k,v]\n",
        "    d2.append(temp)\n",
        "  d1 = find_docs(d1,d2,5)"
      ],
      "metadata": {
        "id": "P3RRoKJQd1v4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"List of Document Names::\")\n",
        "total=0\n",
        "for x in d1:\n",
        "  total=total+1\n",
        "  print(mapping[x[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI4QU3mNeqKy",
        "outputId": "8da6035a-3a68-401c-84bd-45baf9a5914b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of Document Names::\n",
            "practica.txt\n",
            "humor9.txt\n",
            "insult.lst\n",
            "cabbage.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Documents Retrieved::\")\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcG1s_lJfJpa",
        "outputId": "021d8d19-89af-4a26-f6ee-fdb2e7959a85"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents Retrieved::\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NrAr92mzis4W"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}